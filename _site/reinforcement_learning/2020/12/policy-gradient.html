<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Introduction to Policy Gradient Method in Reinforcement Learning | Skylar Lee</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Introduction to Policy Gradient Method in Reinforcement Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Just hanging here." />
<meta property="og:description" content="Just hanging here." />
<link rel="canonical" href="http://localhost:4000/reinforcement_learning/2020/12/policy-gradient.html" />
<meta property="og:url" content="http://localhost:4000/reinforcement_learning/2020/12/policy-gradient.html" />
<meta property="og:site_name" content="Skylar Lee" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-21T00:00:00-08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Introduction to Policy Gradient Method in Reinforcement Learning","dateModified":"2020-12-21T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/reinforcement_learning/2020/12/policy-gradient.html"},"description":"Just hanging here.","url":"http://localhost:4000/reinforcement_learning/2020/12/policy-gradient.html","datePublished":"2020-12-21T00:00:00-08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Skylar Lee" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Skylar Lee</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Introduction to Policy Gradient Method in Reinforcement Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-12-21T00:00:00-08:00" itemprop="datePublished">Dec 21, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p>Policy gradient is a reinforcement learning (RL) algorithm that directly tries to maximize the rewards (as opposed to using another proxy value, e.g., q function). Let’s revisit the goal of reinforcement learning: it’s to maximize the expected cumulative rewards, i.e.,</p>

\[p_\theta (\tau) = p(s_1)\prod^T_{t=1}\pi_\theta(a_t|s_t)p(s_{t+1}|s_t,a_t) \\
\mathop{\operatorname{arg\,max_\theta}} E_{\tau\sim p_\theta(\tau)} \sum_t r(s_t, a_t)\]

<p>We will try to find the best policy by taking the gradient of the objective</p>

\[J(\theta) =  E_{\tau\sim p_\theta(\tau)} \sum_t r(s_t, a_t)\]

<p>We will first go through the varient of policy gradient method called REINFORCE; it uses Monte-Carlo methods in that it learns from observations without modeling the dynamics and uses the mean of observations as the approximation of the expected return.</p>

<p>Recall that the typical steps of training in reinforcement learning consists of repeating 1) generating samples, 2)fitting a model, 3)improving the policy. We can estimate \(J(\theta)\) by running the policy and observing the rewards. For example, if I’m using reinforcement learning to train a polcy plaing an Atari game, I would start by making random moves, get a set of state and action pairs \(s_1, a_1, s_2, a_2\). Our reward function will be the score displayed in the game, and can be mapped as \(r(s_i, a_i)\). It would be a good idea to have a few or more trajectories before we fit a model to get more data; if we had \(N\) trajectories, then</p>

\[J(\theta) = \frac{1}{N} \sum_i^N \sum_t^T r(s_{i,t}, a_{i,t})\]

<p>After some math jugling, you will find the gradient is</p>

\[\triangledown_\theta J(\theta) = \sum_i \bigl( \sum_t \triangledown_\theta \log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \bigr) \bigl( \sum_t r(s_{i,t}, a_{i,t}) \bigr)\]

<p>So you calculate the gradient and update the policy by
\(\theta_{t+1} \leftarrow \theta_t + \alpha \triangledown J(\theta_t)\)</p>

<p>Here’s a sample pseudocode snippet in pytorch:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>

<span class="k">def</span> <span class="nf">batch_train</span><span class="p">(</span><span class="n">observation_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">,</span> <span class="n">hidden_layer_dim</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>

    <span class="n">policy</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">observation_dim</span><span class="p">,</span> <span class="n">hidden_layer_dim</span><span class="p">),</span>
                            <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_layer_dim</span><span class="p">,</span> <span class="n">action_dim</span><span class="p">))</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">NUM_STEPS</span><span class="p">):</span>
        <span class="n">observations</span><span class="p">,</span> <span class="n">actions</span><span class="p">,</span> <span class="n">rewards</span><span class="p">,</span> <span class="n">next_observations</span> <span class="o">=</span> <span class="n">sample</span><span class="p">()</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">observations</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">acts_dist_given_obs</span> <span class="o">=</span> <span class="n">policy</span><span class="p">(</span><span class="n">observations</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">mul</span><span class="p">(</span><span class="n">acts_dist_given_obs</span><span class="p">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">actions</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="mi">1</span><span class="p">)))</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">loss</span><span class="p">.</span><span class="n">backwards</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<p>The policy gradient is nice because we don’t really need to know about distributions of states or the dynamics model \(p(s_{t+1} \vert s_t,a_t)\). It’s also useful when your action space is infinite, because you can actually take the gradient to move towards a better policy. Unlike in policy gradient, in policy iteration methods, the “improving the policy” step will have to iterate through all the possible actions to find one that maximizes the q-value (expected rewards given a state and an action).</p>

<p>The downside is that we need a lot of samples to deal with the variance. Hihg variance means the policy is going to not converge well and we will spend a lot of time training. However, without sampling more, there are some tricks to help reduce the variance.</p>

<h3 id="reducing-variance">Reducing variance</h3>

<h4 id="using-rewards-to-go">Using Rewards-To-Go</h4>
<p>Take a look at our policy gradient.</p>

\[\triangledown_\theta J(\theta) = \sum_i \bigl( \sum_{t=1}^T \triangledown_\theta \log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \bigr) \bigl( \sum_{t=1}^T r(s_{i,t}, a_{i,t}) \bigr)\]

<p>We know that an action taken at \(t'\) cannot affect rewards at \(t &lt; t'\). So we can skip adding it to our gradient. By virtue of having fewer items to sum, we can reduce variance. Witih this modification, our gradient becomes</p>

\[\triangledown_\theta J(\theta) = \sum_i \bigl( \sum_t \triangledown_\theta \log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \bigr) \bigl( \sum_{t'=t}^T r(s_{i,t'}, a_{i,t'}) \bigr)\]

<p>In our code, we will use <code class="language-plaintext highlighter-rouge">rewards_to_go</code> instead of <code class="language-plaintext highlighter-rouge">torch.sum(rewards, 1)</code></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">rewards_to_go</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">rewards</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">rewards</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">rewards_to_go</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">[:,</span> <span class="n">t</span><span class="p">:])</span>
</code></pre></div></div>

<h4 id="using-baseline">Using Baseline</h4>
<p>Another idea is to subtract a baseline. This will reduce variance, while keeping the bias unchanged because
\(E_{a_t ~ \pi(\theta)}\bigl[ \triangledown_\theta \log \pi_\theta(a_t \vert s_t)b(s_t) \bigr] = 0\).</p>

<p>We could use the average rewards \(b = \frac{1}{N}\sum_i^N \sum_{t'=t}^T r(s_t', a_t')\) from the policy \(\pi\), or train a simple neural net that predicts rewards based on observation. Our gradient will now be</p>

\[\triangledown_\theta J(\theta) = \sum_i \bigl( \sum_t \triangledown_\theta \log \pi_{\theta} (a_{i,t} \vert s_{i,t}) \bigr) \bigl( \sum_{t'=t}^T r(s_{i,t'}, a_{i,t'}) \bigr)\]

<p>These tricks will be a good motivator for the Actor-Critic method, and I’ll discuss that next time!</p>

<!-- // how do we improve variance? what if good samples have 0?
// how to reduce variance? use rewards to go, subtract a baseline (average reward)
https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html
https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d
https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html
https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html#monte-carlo-methods -->

  </div><a class="u-url" href="/reinforcement_learning/2020/12/policy-gradient.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Skylar Lee</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Skylar Lee</li><li><a class="u-email" href="mailto:me@skylarlee.dev">me@skylarlee.dev</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Just hanging here.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
