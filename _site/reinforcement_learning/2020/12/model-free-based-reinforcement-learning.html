<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Model free vs Model-based Reinforcement Learning | Skylar Lee</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Model free vs Model-based Reinforcement Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Just hanging here." />
<meta property="og:description" content="Just hanging here." />
<link rel="canonical" href="http://localhost:4000/reinforcement_learning/2020/12/model-free-based-reinforcement-learning.html" />
<meta property="og:url" content="http://localhost:4000/reinforcement_learning/2020/12/model-free-based-reinforcement-learning.html" />
<meta property="og:site_name" content="Skylar Lee" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-21T00:00:00-08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Model free vs Model-based Reinforcement Learning","dateModified":"2020-12-21T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/reinforcement_learning/2020/12/model-free-based-reinforcement-learning.html"},"description":"Just hanging here.","url":"http://localhost:4000/reinforcement_learning/2020/12/model-free-based-reinforcement-learning.html","datePublished":"2020-12-21T00:00:00-08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Skylar Lee" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Skylar Lee</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Model free vs Model-based Reinforcement Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2020-12-21T00:00:00-08:00" itemprop="datePublished">Dec 21, 2020
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p>There are many different reinforcement learning (RL) algorithms. One of the ways you can categorize RL algorithms is whether they use a model or not. You can think of a model as understanding of how the system changes when you perform an action. For example, suppose you are picking up an object from one place and drop it in another place. You understand the physics and can guess where your hand will be when you moved it right. That understading of the state change is equivalent to “model” in reinforcement learning.</p>

<p>Going back to the markov diagram, we can say the probability of seeing a particular rollout is \(p(s_1)\prod^T*{t=1}\pi*\theta(a_t%7Cs_t)p(s_{t+1}%7Cs_t,a_t)\). Here the model is \(p(s_{t+1}%7Cs_t,a_t)\), transition probability distribution.</p>

<h3 id="model-free">Model-free</h3>

<p>In the model-free approach, we do not try to learn \(p(s_{t+1}%7Cs_t,a_t)\); we just watch the states change during the sampling phase. \(\tau\) is just given to us.</p>

<h3 id="model-based">Model-based</h3>

<p>In the model-based approach, we try to learn this transition probability distribution and use it in improving your policy as well.</p>

<p>So why would one use a model-free vs model-based algorithm, and why are there different approaches?</p>

<h3 id="sampling-efficiency">Sampling efficiency</h3>

<p>Needing fewer samples to train a good policy is an advantage. Here, model-based reinforcement learning has advantage; since you will learn how the state changes given action as part of the algorithm, you don’t need to generate new samples when the policy changes. For example, if you were training a policy that moves an object with a robot arm, and your policy put the arm in a place it’s never been, you would want to run a physical simulation or have the arm move in the real environment to make an observation of what happens, e.g., it bumps into the wall, knocks off another object, etc.
Humans gain understanding of physics, so we can guess that when our hand moves and makes contact with an object with enough momentum, e.g., a vase, we will knock it down without ever having to do that ourselves. (Maybe young humans don’t, because they are still busy generating samples…)</p>

<h3 id="stability-and-training-efficiency">Stability and Training Efficiency</h3>

<p>Stability is a big issue in reinforcement learning. In supervised learning, we optimize the objective function almost always with gradient descent. In reinforcement learning, we often don’t; we use other approximations, such as with value functions (what’s the expected total reward in a given state), how well the model fits.
Here, it’s hard to say one is better than the other. Model-based RL algorithm has the issue that we’re optimizing for the fit of the model with gradient descent, rather than the rewards, the true objective of reinforcement learning, and a good model doesn’t necessarily result in high rewards. Policy gradient method, which is model-free and optimizes the rewards directly, tends to converge very slowly in many cases.
Model-based learning has been applied in robotics and seen some success, as there has already been a lot of success in computer vision, such as <img src="https://dl.acm.org/doi/abs/10.5555/2946645.2946684" alt="this paper" />, but that’s not to say model-free approach was not successful in robotics (note <img src="https://arxiv.org/abs/1806.10293" alt="QT-Opt" />)</p>


  </div><a class="u-url" href="/reinforcement_learning/2020/12/model-free-based-reinforcement-learning.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Skylar Lee</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Skylar Lee</li><li><a class="u-email" href="mailto:me@skylarlee.dev">me@skylarlee.dev</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Just hanging here.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
