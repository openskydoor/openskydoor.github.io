<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Actor-Critic Methods in Reinforcement Learning | Skylar Lee</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Actor-Critic Methods in Reinforcement Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Just hanging here." />
<meta property="og:description" content="Just hanging here." />
<link rel="canonical" href="http://localhost:4000/reinforcement_learning/2021/01/actor-critic.html" />
<meta property="og:url" content="http://localhost:4000/reinforcement_learning/2021/01/actor-critic.html" />
<meta property="og:site_name" content="Skylar Lee" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-09T00:00:00-08:00" />
<script type="application/ld+json">
{"@type":"BlogPosting","headline":"Actor-Critic Methods in Reinforcement Learning","dateModified":"2021-01-09T00:00:00-08:00","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/reinforcement_learning/2021/01/actor-critic.html"},"description":"Just hanging here.","url":"http://localhost:4000/reinforcement_learning/2021/01/actor-critic.html","datePublished":"2021-01-09T00:00:00-08:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/main.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Skylar Lee" /></head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Skylar Lee</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Actor-Critic Methods in Reinforcement Learning</h1>
    <p class="post-meta">
      <time class="dt-published" datetime="2021-01-09T00:00:00-08:00" itemprop="datePublished">Jan 9, 2021
      </time></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<p>In my previous <a href="/reinforcement_learning/2020/12/reinforcement-learning-primer-rewards.html">post</a> on reinforcement learning (RL) based on rewards, I mentioned the true objective of reinforcement function is find the parameters of your policy such that you maximize the expected value of the total rewards, i.e., \(\mathop{\operatorname{arg\,max_\theta}} E_{\tau\sim p_\theta(\tau)} \sum_t r(s_t, a_t)\). For example, you have come up with some architecture for your model, e.g., your neural net, and then during the training phase, you need to find the right numbers for the matrices (parameters). In each optimization step during the training, you need to update your parameters a bit by bit based on your objective function. Our true object has quite a few values to calculate, so computer scientists have come up with some other objective functions that are nicer to use. // list why value/q functions.</p>

<p>Let’s define a couple of value functions.</p>

<h2 id="q-function">Q-function</h2>

<p><em>Q-function (\(Q(s_t, a_t)\))</em> provides total reward from taking \(a_t\) in \(s_t\).</p>

<p>To help understanding the relationship of this to the RL objective, let me give you an example. If we knew the reward of taking action \(a_1\) in state \(s_1\), and understood the probability distribution of taking \(a_1\) in \(s_1\) and that of the state \(s_1\), then, the RL objective can be rewritten as</p>

<table>
  <tbody>
    <tr>
      <td>$$E_{s_1 ~ p(s_1)} [E_{a_1 ~ \pi(a_1 \vert s_1)}[Q(s_1, a_1)</td>
      <td>s_1]]$$.</td>
    </tr>
  </tbody>
</table>

<p>This will be an important fact for motivating Q-learning later.</p>

<h2 id="value-function">Value function</h2>

<table>
  <tbody>
    <tr>
      <td><em>Value function (\(V(s_t)\))</em> provides total reward from \(s_t\). Note that it doesn’t depend on the action, so that it should sum Q-values over all the possible actions per $$a_t ~ \pi(a_t</td>
      <td>s_t)$$. The RL objective can be rewritten as</td>
    </tr>
  </tbody>
</table>

<p>\(E_{s_1 ~ p(s_1)}[V(s_1)]\).</p>

<h3 id="actor-critic">Actor-Critic</h3>

<p>Recall in the policy gradient method (related <a href="/reinforcement_learning/2020/12/policy-gradient.html">post</a>, the gradient that we used to update the policy parameters is</p>

<p>$$</p>

<p>// talk about policy gradient
// high variance with small sample
// import with baseline, rewards to go
// estimate by fitting sampled reward sums
//
An actor-critic method is like a hybrid of using policy gradients and value functions.</p>

<p>Instead of fitting the RL objective directly, we fit either \(V(s)\) or \(Q(s, a)\), and update the</p>

<p>estimate value function or Q-function of the current policy,
use it to improve policy</p>


  </div><a class="u-url" href="/reinforcement_learning/2021/01/actor-critic.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <h2 class="footer-heading">Skylar Lee</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Skylar Lee</li><li><a class="u-email" href="mailto:me@skylarlee.dev">me@skylarlee.dev</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/jekyll"><svg class="svg-icon"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg> <span class="username">jekyll</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>Just hanging here.</p>
      </div>
    </div>

  </div>

</footer>
</body>

</html>
